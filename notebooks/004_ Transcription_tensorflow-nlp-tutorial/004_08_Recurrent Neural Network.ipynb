{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "The feed-forward neural networks we learned about earlier have fixed input lengths, which limits them as neural networks for natural language processing. Eventually, we needed artificial neural networks that could handle input sequences of varying lengths, and the most common artificial neural network used for natural language processing is the recurrent neural network (RNN). In this chapter, we'll learn about the most basic recurrent neural network, the vanilla RNN, and its improvements, the LSTM and GRU. Understanding LSTMs and GRUs can help you solve a variety of natural language processing problems, such as text categorization and machine translation.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Recurrent Neural Network, RNN\n",
    "\n",
    "Recurrent neural networks (RNNs) are sequence models that process inputs and outputs as sequences. Think of a translator: the input is a sentence, which is a sequence of words to be translated, and the output is a translated sentence, which is also a sequence of words. Models designed to process sequences like this are called sequence models, and RNNs are the most basic artificial neural network sequence model.\n",
    "\n",
    "LSTMs and GRUs, which we learn about later, are also essentially RNNs. You will understand RNNs and practice them in the \"Text Classification with RNNs\" chapter, \"Tagging Tasks\" chapter, and \"Encoder-Decoder with RNNs\" chapter.\n",
    "\n",
    "_Although the terminology is similar, Recurrent Neural Networks and Recursive Neural Networks are two different things._\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Recurrent Neural Network, RNN\n",
    "\n",
    "All of the neural networks we've seen so far have activation functions in the hidden layer, and the values past the activation function are only directed towards the output layer. These neural networks are called feed forward neural networks. However, there are other neural networks that are not. Recurrent neural networks (RNNs) are one of them. RNNs are characterized by sending outputs from activation functions on nodes in the hidden layer to the output layer, which in turn are used as inputs to the next calculation on the hidden layer nodes.\n",
    "\n",
    "![](https://wikidocs.net/images/page/22886/rnn_image1_ver2.PNG)\n",
    "\n",
    "Consider the figure above. $x$ is the input vector of the input layer, $b$ is the output vector of the output layer. In reality, the bias can also be present as an input, but we will omit it in the following illustrations. In an RNN, the nodes that are responsible for outputting results from the hidden layer through the activation function are called cells. These cells act as a kind of memory that tries to remember previous values, so we refer to them as memory cells or RNN cells.\n",
    "\n",
    "![](https://wikidocs.net/images/page/22886/rnn_image2_ver3.PNG)\n",
    "\n",
    "\n",
    "![](https://wikidocs.net/images/page/22886/rnn_image2.5.PNG)\n",
    "\n",
    "\n",
    "![](https://wikidocs.net/images/page/22886/rnn_image3_ver2.PNG)\n",
    "\n",
    "\n",
    "![](https://wikidocs.net/images/page/22886/rnn_image3.5.PNG)\n",
    "\n",
    "\n",
    "![](https://wikidocs.net/images/page/22886/rnn_image3.7.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
